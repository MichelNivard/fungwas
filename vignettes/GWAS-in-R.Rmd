---
title: "tips & ideas for GWAS in R"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{tips & ideas for GWAS in R}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(fungwas)
```


## Why this vignette?

You’ve got genotypes (SNPs), covariates (age/sex/PCs), and a phenotype. You want to:

1. Load a chunk of variants into R without blowing up memory
2. Keep the SNP metadata (ID, chr, bp, alleles) **near the columns**
3. Run `quantile_gwas()` once per chunk, then reuse it with different weight systems via `param_gwas()`
4. Optionally parallelize across chunks

This vignette shows pragmatic routes from **PLINK/VCF → R → fungwas**, with minimal fuss.



## File formats and quick import options

* **PLINK bed/bim/fam**: classical; use PLINK2 to slice/export easily.
* **VCF/BCF**: also fine; use `seqminer::tabix.read()` or convert to PLINK2 PGEN/RAW first.
* **Direct R import**:

  * `genio` reads bed/bim/fam into R objects.
  * `seqminer` reads bgzipped VCF by region (Tabix).
  * `bigstatsr/bigsnpr` are excellent for very large data (memory-mapped); optional.

Pick one route and stay consistent.

---

## Minimal PLINK2 route (recommended)

Suppose you have chromosome 1 in PLINK binary:
`genotype_chr1.fam`, `genotype_chr1.bim`, `genotype_chr1.bed`.

To extract a **3 Mb region** (1–3,000,000 bp) as plain text dosages:

```bash
plink2 \
  --bfile genotype_chr1 \
  --chr 1 \
  --from-bp 1 \
  --to-bp 3000000 \
  --export A \
  --out genotype_scatter_chr1_1_3000000
```

This creates `genotype_scatter_chr1_1_3000000.raw` containing additive dosages (REF-coded by default; change with `--export-allele` if needed). The `.raw` columns look like:

* First columns: `FID IID PAT MAT SEX PHENOTYPE`
* Then one column per SNP, typically named like `SNPID_A1` (PLINK appends the counted allele)

**Load the `.raw` file in R**:

```{r eval=FALSE}
library(data.table)
raw <- fread("genotype_scatter_chr1_1_3000000.raw")
head(raw[, 1:8])
```

You can peel off IDs and convert the genotype columns to a numeric matrix:

```{r eval=FALSE}
id_cols <- c("FID","IID","PAT","MAT","SEX","PHENOTYPE")
geno_dt <- raw[, !..id_cols]
G <- as.matrix(geno_dt)               # N x P

# keep SNP column names (often "rsid_A1")
snp_cols <- colnames(geno_dt)
```

> **Tip**: Keep a **SNP metadata table** alongside columns (from `.bim` or your own index):
>
> ```r
> bim <- fread("genotype_chr1.bim", col.names = c("chr","snp","cm","bp","a1","a2"))
> # Restrict to region and in the same order as 'snp_cols'
> bim_sub <- bim[bp >= 1 & bp <= 3e6]
> # Match to snp_cols after PLINK’s allele suffixing
> # e.g., map from 'snp' + '_A1' to column names
> bim_sub[, plink_col := paste0(snp, "_", a1)]
> bim_sub <- bim_sub[match(snp_cols, plink_col)]
> stopifnot(all(bim_sub$plink_col == snp_cols))  # sanity
> ```

This ensures **SNP info stays aligned** with the genotype matrix columns.

---

## Covariates and PCs

* **From FAM**: you get `IID`, `SEX`, and a placeholder phenotype.
* **PCs**: read from a separate file and **merge by IID**.
* Ensure the **row order of `G`, covariates `C`, and phenotype `Y` is identical**.

```{r eval=FALSE}
fam <- fread("genotype_chr1.fam", col.names = c("FID","IID","PAT","MAT","SEX","PHENO"))
pcs <- fread("pcs.tsv")   # columns: IID, PC1..PC20
covs <- merge(fam[, .(IID, SEX)], pcs, by = "IID", all.x = TRUE)

# Reorder covs to match the .raw row order (IID in 'raw')
covs <- covs[match(raw$IID, covs$IID)]
stopifnot(all(covs$IID == raw$IID))

# Build C matrix: intercept is handled internally, so do not include it here
C <- as.matrix(covs[, .(SEX, PC1, PC2, PC3, PC4, PC5)])
```

---

## Chunking strategy

Don’t try to load all SNPs across all chromosomes at once. Pick a **chunking unit**:

* **By region** (e.g. 3–10 Mb windows)
* **By SNP count** (e.g. 25k–100k SNPs per chunk)

**Heuristics**:

* Memory for `G` ≈ N × P × 8 bytes (double). For `N=200k, P=50k` ⇒ ~80 GB (too big).

  * Use `single` precision (`storage.mode(G) <- "double"` is required in our code though), or process smaller P per chunk.
  * Consider **bigsnpr** for memory-mapped matrices if extremely large.
* Time scales with `O(N × P × |taus|)` inside `quantile_gwas()`.
* A practical starting point: **N up to 200k, P ≈ 10k–50k per chunk, |taus| ≈ 15–25**.
* Parallelize across chunks (not within) to keep code simple.

---

## A solid end-to-end example (one chunk)

Assume you’ve loaded **one region** into `G`, plus `Y` and `C`.

```{r eval=FALSE}
# 1) Choose quantiles and run Stage 1 once per chunk
taus  <- seq(0.10, 0.90, by = 0.05)
stage1 <- quantile_gwas(
  Y, G,
  taus = taus,
  C = C,
  residualize_Y = FALSE,   # default: only SNPs are residualized on C
  benchmark = TRUE
)

# 2a) Map to mean/variance (vQTL)
W_var <- make_weight_vqtl(taus, stage1$q_tau, mu = mean(Y), sd = sd(Y))
fit_var <- param_gwas(stage1, transform = "custom_W", transform_args = list(W = W_var),
                      se_mode = "plugin_cor")

# 2b) Map to two-component mixture (means + membership)
fit_mix <- param_gwas(
  stage1,
  transform = "two_normal",
  transform_args = list(
    p1 = 0.5, mu1 = 1.2, sd1 = 0.45,
    mu2 = 3.0, sd2 = 0.7,
    include_membership = TRUE
  ),
  se_mode = "diagonal"
)

# 3) Keep SNP info alongside results
est_var <- data.table(bim_sub[, .(chr, snp, bp, a1, a2)], t(fit_var$params))
est_mix <- data.table(bim_sub[, .(chr, snp, bp, a1, a2)], t(fit_mix$params))

# Example preview
head(est_var)
head(est_mix)
```

**Why two stages?** You can reuse `stage1` to test *multiple* parametric systems (e.g., vQTL, mixture, log-normal) **without re-running** the heavy SNP × τ loop.

---

## Parallelism across chunks

The simplest approach: **split the genome** into chunks (by region or by SNP count), and run each chunk in a separate process. Here are two lightweight options.

### Option A: `future.apply`

```{r eval=FALSE}
library(future.apply)
plan(multisession, workers = 4)  # or multicore on Linux

chunk_specs <- data.table(
  chr = 1,
  from_bp = seq(1, 2.5e8, by = 5e6),
  to_bp   = pmin(seq(5e6, 2.5e8 + 5e6, by = 5e6), 2.5e8)
)

run_chunk <- function(chr, from_bp, to_bp) {
  # 1) Use plink2 to export .raw for the region (system call)
  outpref <- sprintf("chr%s_%d_%d", chr, from_bp, to_bp)
  cmd <- sprintf("plink2 --bfile genotype_chr%s --chr %s --from-bp %d --to-bp %d --export A --out %s",
                 chr, chr, from_bp, to_bp, outpref)
  system(cmd)

  # 2) Read .raw, .bim subset; build G and SNP meta
  raw  <- fread(paste0(outpref, ".raw"))
  fam  <- fread(paste0("genotype_chr", chr, ".fam"),
                col.names = c("FID","IID","PAT","MAT","SEX","PHENO"))
  bim  <- fread(paste0("genotype_chr", chr, ".bim"),
                col.names = c("chr","snp","cm","bp","a1","a2"))

  # region meta
  meta <- bim[bp >= from_bp & bp <= to_bp]

  # genotype matrix
  id_cols <- c("FID","IID","PAT","MAT","SEX","PHENOTYPE")
  geno_dt <- raw[, !..id_cols]
  G <- as.matrix(geno_dt)
  snp_cols <- colnames(geno_dt)

  # align meta to columns (assuming PLINK col names as snp_a1)
  meta[, plink_col := paste0(snp, "_", a1)]
  meta <- meta[match(snp_cols, plink_col)]
  stopifnot(all(meta$plink_col == snp_cols))

  # 3) Merge PCs/covariates and phenotype in the same row order as 'raw'
  pcs  <- fread("pcs.tsv")    # has IID, PC1..PC20
  covs <- merge(fam[, .(IID, SEX)], pcs, by = "IID", all.x = TRUE)
  covs <- covs[match(raw$IID, covs$IID)]
  C <- as.matrix(covs[, .(SEX, PC1, PC2, PC3, PC4, PC5)])

  # 4) Pull phenotype Y in the same order; replace with your source
  pheno <- fread("pheno.tsv") # IID, Y
  pheno <- pheno[match(raw$IID, pheno$IID)]
  Y <- pheno$Y

  # 5) Quantile GWAS and one mapping (you can add more)
  taus   <- seq(0.10, 0.90, 0.05)
  stage1 <- quantile_gwas(Y, G, taus = taus, C = C, benchmark = FALSE)

  W_var  <- make_weight_vqtl(taus, stage1$q_tau, mu = mean(Y), sd = sd(Y))
  fit    <- param_gwas(stage1, transform = "custom_W", transform_args = list(W = W_var))

  # 6) Return a data.table with meta + estimates
  out <- data.table(meta[, .(chr, snp, bp, a1, a2)], t(fit$params))
  out[]
}

results_list <- future_mapply(
  run_chunk,
  chr = chunk_specs$chr,
  from_bp = chunk_specs$from_bp,
  to_bp = chunk_specs$to_bp,
  SIMPLIFY = FALSE
)

results <- rbindlist(results_list, use.names = TRUE, fill = TRUE)
fwrite(results, "fungwas_vqtl_results.tsv")
```

### Option B: `foreach` + `doParallel`

Works similarly; pick whichever your cluster policy prefers.

---

## VCF input alternative (no PLINK2)

If you have bgzipped VCF/BCF with Tabix index, you can slice regions directly:

```{r eval=FALSE}
library(seqminer)

region <- "1:1-3000000"
vcf_file <- "chr1.vcf.bgz"

# dosage extraction (depends on FORMAT fields and your VCF)
geno <- seqminer::tabix.read(vcf_file, region)
# Convert to an N x P dosage matrix 'G' for your samples
# Build SNP meta from INFO/ID/POS/REF/ALT in 'geno'
```

This route is great if you already distribute data as indexed VCF per chromosome.

---

## Practical notes

* **Allele coding**: PLINK2 `--export A` counts the **REF** allele by default. If you prefer effect allele to be ALT, set `--export-allele` accordingly, and keep that consistent when interpreting signs.
* **Sample order**: Always reorder covariates and phenotype to match the genotype rows.
* **QC**: This vignette assumes you’ve already done SNP/sample QC (missingness, MAF, HWE, ancestry, duplicates).
* **Chunk size**: Aim for total memory ( \approx ) 2–8 GB per worker to be safe; test on one chunk and scale.
* **Stage reuse**: Save `stage1` objects per chunk (e.g. `saveRDS(stage1, "chunkX_stage1.rds")`) so you can rerun different `param_gwas()` analyses later without repeating Stage 1.

---

## Interpreting results

* `quantile_gwas()` returns **τ-slope matrices** (`Q_slope`, `SE_tau`) and baseline quantiles/densities.
* `param_gwas()` returns per-SNP effects on your chosen parameters and delta-method SEs:

  * vQTL: `beta_mu`, `beta_sigma2`
  * Mixture: `gamma` (membership), `beta_1`, `beta_2` (component means)
  * Mixture vQTL: add `beta_sigma1`, `beta_sigma2`
* Keep a joined table **[SNP meta | parameter estimates | SEs]** for clean downstream work.

---

## Summary

* Use PLINK2 (or Tabix/VCF) to export **chunked** genotype matrices.
* Keep SNP metadata aligned with columns.
* Run **Stage 1 once per chunk**; reuse for multiple parameter systems.
* Parallelize **across chunks**; keep each worker’s memory footprint modest.
* Store results with SNP meta for painless interpretation and plotting.

